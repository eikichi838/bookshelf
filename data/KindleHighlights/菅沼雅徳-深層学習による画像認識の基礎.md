---
kindle-sync:
  bookId: '46563'
  title: 深層学習による画像認識の基礎
  author: 菅沼雅徳
  asin: B0D2KFXWK8
  lastAnnotatedDate: Invalid date
  bookImageUrl: 'https://m.media-amazon.com/images/I/81Gjm-UiAIL._SY160.jpg'
  highlightsCount: 51
---
# 深層学習による画像認識の基礎
## Metadata
* Author: [菅沼雅徳](https://www.amazon.comundefined)
* ASIN: B0D2KFXWK8
* Reference: https://www.amazon.com/dp/B0D2KFXWK8
* [Kindle link](kindle://book?action=open&asin=B0D2KFXWK8)

## Highlights
https://github.com/sg-nm/image-recognition — location: [2624](kindle://book?action=open&asin=B0D2KFXWK8&location=2624) ^ref-21211

---
それぞれ画像分類 （image classification）， 物体検出 （object detection）， 意味 的領域分割 （semantic segmentation） などとして言及されます． — location: [5682](kindle://book?action=open&asin=B0D2KFXWK8&location=5682) ^ref-58483

---
ネットワークエンジニアリング — location: [8302](kindle://book?action=open&asin=B0D2KFXWK8&location=8302) ^ref-3242

---
機械学習とは 最後に，機械学習の概念を簡単に説明します． — location: [8305](kindle://book?action=open&asin=B0D2KFXWK8&location=8305) ^ref-9465

---
Lpノルム — location: [12236](kindle://book?action=open&asin=B0D2KFXWK8&location=12236) ^ref-64749

---
ラベ ルのエントロピーは0であるためこの項は無視できます． — location: [13547](kindle://book?action=open&asin=B0D2KFXWK8&location=13547) ^ref-59526

---
するスキップ接続 （skip connection） による残 差学習 （residual learning） [8] です． — location: [24031](kindle://book?action=open&asin=B0D2KFXWK8&location=24031) ^ref-47331

---
学習率のスケジューリング — location: [26652](kindle://book?action=open&asin=B0D2KFXWK8&location=26652) ^ref-8531

---
重み減衰 （weight decay） — location: [27092](kindle://book?action=open&asin=B0D2KFXWK8&location=27092) ^ref-21208

---
層 — location: [34954](kindle://book?action=open&asin=B0D2KFXWK8&location=34954) ^ref-47420

---
正規化 （layer normalization） [22] は，サンプルごとに，チ ャネル次元に沿って統計量を算出し，正規化します．層正規化はミニバッチ数 に依存しないため，上述したミニバッチサイズが小さいことによる問題は生じ ません．ただし，導入されるモデルに応じて有効性が変わる点には注意が必要 です．例えば，後述する ResNet [8] に層正規化を導入しても有効性はあまり 得られませんが，よりモダンな CNN [41]やVision Transformer [42] では，バ ッチ正規化よりも優れた性能を示すことが報告されています． — location: [34954](kindle://book?action=open&asin=B0D2KFXWK8&location=34954) ^ref-61811

---
なお，関数Fl (·) に 2から 3層程度の畳込み層を採用する理由は，仮に 1層の 畳込み層だけを採用した場合，残差ブロックの演算はXl+1 = Xl + Wl 1Xl と なり，線形変換に近い形になってしまい，表現力が落ちてしまうためです．残 差学習および残差ブロックの詳細については， 3.6.1項で改めて説明します． — location: [35828](kindle://book?action=open&asin=B0D2KFXWK8&location=35828) ^ref-13288

---
伝統的な CNNにおけるプーリング層では，ストライドs = 2, パディング p = 0， カーネルサイズ2 × 2 の最大プーリングを用いて，特徴マップの解像 度を縦横それぞれ1/2 にすることが多いです．また，プーリング処理後の特 徴マップに対して，正規化処理や活性化関数は適用しないのが通例です． — location: [36701](kindle://book?action=open&asin=B0D2KFXWK8&location=36701) ^ref-21417

---
大域平均プーリング — location: [36701](kindle://book?action=open&asin=B0D2KFXWK8&location=36701) ^ref-38941

---
CIFAR-10のような画像分類においては，プーリング処理は強すぎる帰納バイ アスであり，畳込み層による代替で十分であると結論付けています．実際のと ころ，近年使用されている CNNの多くは，プーリング層を用いず，ストライド が2以上の畳込み層で代替しています． — location: [37577](kindle://book?action=open&asin=B0D2KFXWK8&location=37577) ^ref-58689

---
上 — location: [38013](kindle://book?action=open&asin=B0D2KFXWK8&location=38013) ^ref-43560

---
記の問題を踏まえて，近年の CNNでは全結合層の直前に GAP を挿入す ることが一般的です． GAP によって，いかなる空間解像度であっても， C × 1 × 1サイズの特徴マップに変換できるため，全結合層のパラメータ数を減ら しつつ，任意サイズの画像が入力可能となります． — location: [38013](kindle://book?action=open&asin=B0D2KFXWK8&location=38013) ^ref-53865

---
EfficientNet — location: [45439](kindle://book?action=open&asin=B0D2KFXWK8&location=45439) ^ref-48632

---
ConvNeXt — location: [48934](kindle://book?action=open&asin=B0D2KFXWK8&location=48934) ^ref-23928

---
多くのCNN に共通する設計指針 — location: [50246](kindle://book?action=open&asin=B0D2KFXWK8&location=50246) ^ref-41238

---
図4.6 (a)事後層正規化 （Transformerの原論文で採用）． (b)事前層正 規化 （ViTやBERT， GPTなどで採用）． — location: [55488](kindle://book?action=open&asin=B0D2KFXWK8&location=55488) ^ref-64309

---
Swin Transformer — location: [62480](kindle://book?action=open&asin=B0D2KFXWK8&location=62480) ^ref-31898

---
Swin Transformerの概要図． — location: [62915](kindle://book?action=open&asin=B0D2KFXWK8&location=62915) ^ref-62016

---
ViTのメタアーキテクチャ． — location: [71216](kindle://book?action=open&asin=B0D2KFXWK8&location=71216) ^ref-41996

---
図5.2 CNNによる物体検出器の概要．まずは，バックボーンによって 入力画像からマルチスケールの画像特徴を抽出します．そして， 抽出された画像特徴をネックで洗練化します．最後に，各スケー ルの画像特徴に対して，ヘッドを用いて各ボックスの位置と該当 するクラスラベルを推定します． — location: [74712](kindle://book?action=open&asin=B0D2KFXWK8&location=74712) ^ref-54892

---
特徴ピラミッド ネットワーク （feature pyramid network; FPN） [168] を用いて，マルチスケ ールの特徴マップを利用するのが標準的です．本項では， 2024年3月現在も 利用されている Faster R-CNNについて解説します*3 — location: [75151](kindle://book?action=open&asin=B0D2KFXWK8&location=75151) ^ref-58932

---
Intersection over Union （IoU） — location: [77334](kindle://book?action=open&asin=B0D2KFXWK8&location=77334) ^ref-16671

---
DETRの全体像． DETR はバックボーン， Tranformerエンコ ーダ，デコーダ，検出ヘッドで構成されます． — location: [82576](kindle://book?action=open&asin=B0D2KFXWK8&location=82576) ^ref-18874

---
バックボーンからの出力特徴マップF に対して， 1 × 1畳込み層を適用し， チャネル次元を削減した特徴マップZ0 ∈ Rd×H×W を生成します． — location: [82578](kindle://book?action=open&asin=B0D2KFXWK8&location=82578) ^ref-3656

---
5.4.1 平均適合率 — location: [90877](kindle://book?action=open&asin=B0D2KFXWK8&location=90877) ^ref-38023

---
IoU ごとのmAP また，検出の判定に用いる IoUのしきい値η をいくつか変 更した場合のmAP を報告するのが通例です．一般的には， IoU = 0.5, 0.75 の 場合 （AP50, AP75） と， IoUのしきい値を 0.5から0.95 の範囲で0.05刻みで 変えたときのmAP をそれぞれ算出し，それらを平均したものを AP もしくは mAP として報告します． — location: [91752](kindle://book?action=open&asin=B0D2KFXWK8&location=91752) ^ref-22792

---
物体サイズごとのmAP さらに，正解ボックスの面積をもとに，小物体 （small）， 中物体 （medium）， 大物体 （large） と分類し*32， それぞれに対する mAP を APS, APM, APL とします．これらの指標は，物体検出器の各物体サ イズに対する性能指標として用いられます． — location: [91752](kindle://book?action=open&asin=B0D2KFXWK8&location=91752) ^ref-7184

---
*32 面積322以下の物体を小物体， 322 ∼ 962 サイズの物体を中物体， 962以上のサイズの物体を大 物体と定義します． — location: [91754](kindle://book?action=open&asin=B0D2KFXWK8&location=91754) ^ref-57572

---
DETRの小物体に対する検出精度は，拡大畳込み層を導入したバックボー ン （R50-DC5） を用いることで，ある程度改善可能です． Pix2Seq も DETR と同様にFPN を用いないモデルですが，小物体に対しても優れた性能を示し ているのは特筆すべき点といえます． — location: [92190](kindle://book?action=open&asin=B0D2KFXWK8&location=92190) ^ref-46524

---
（意味的）領域分割では，画像分類や物体検出と異なり，モデルの最終層に 全結合層を使用しないことが肝要です[204]． 領域分割モデルの出力は画像で あるため，全結合層を用いると，空間情報が失われてしまい，かつ任意のサイ ズの入力画像が扱いづらくなります．よって，畳込み層で対象クラス数と同数 のチャネルをもつ特徴マップ（ロジット）を出力することが一般的です． — location: [95247](kindle://book?action=open&asin=B0D2KFXWK8&location=95247) ^ref-17308

---
U-Net — location: [96122](kindle://book?action=open&asin=B0D2KFXWK8&location=96122) ^ref-35562

---
U-Netの全体像． U-Net は標準的なエンコーダ・デコーダ型の CNNモデルですが，エンコーダ内の各ブロックからの出力を対 応するデコーダへの入力に結合する構造が特徴です．特徴マップ のh， w はそれぞれ画像の縦幅，横幅で， c はチャネル数を示し ています． — location: [96557](kindle://book?action=open&asin=B0D2KFXWK8&location=96557) ^ref-32596

---
ダウンサンプリング処理では，ストライドを 2 に設定 した2 × 2最大値プーリングや畳込み層を用いることで，解像度を 1/2 に下げ ることが一般的です．ダウン — location: [96558](kindle://book?action=open&asin=B0D2KFXWK8&location=96558) ^ref-35853

---
アップサンプリング処理は， 3.5.5項で紹介したア ップサンプリングとバイリニア補間，もしくは転置畳込みによって実現しま — location: [96559](kindle://book?action=open&asin=B0D2KFXWK8&location=96559) ^ref-17556

---
す．後続の畳込み層は，エンコーダと同様に標準的な畳込み層もしくは残差ブ ロックが用いられます． — location: [96994](kindle://book?action=open&asin=B0D2KFXWK8&location=96994) ^ref-62938

---
U-Netの擬似コード． — location: [96996](kindle://book?action=open&asin=B0D2KFXWK8&location=96996) ^ref-15086

---
import torch import torch.nn as nn class UNet(nn.Module): def --init--(self, in-channels, out-channels): #Encoder self.enc1 = self.conv-block(in-channels, 64) — location: [96996](kindle://book?action=open&asin=B0D2KFXWK8&location=96996) ^ref-24597

---
self.enc2 = self.conv-block(64, 128) self.enc3 = self.conv-block(128, 256) self.pool = nn.MaxPool2d(kernel-size=2, stride=2) #Bottleneck self.bottleneck = self.conv-block(256, 512) #Decoder self.upconv3 = nn.ConvTranspose2d(512, 256, kernel-size=2, stride=2) self.dec3 = self.conv-block(512, 256) self.upconv2 = nn.ConvTranspose2d(256, 128, kernel-size=2, stride=2) self.dec2 = self.conv-block(256, 128) self.upconv1 = nn.ConvTranspose2d(128, 64, kernel-size=2, stride=2) self.dec1 = self.conv-block(128, 64) self.final-conv = nn.Conv2d(64, out-channels, kernel-size=1) def forward(self, img): #Encoder Path enc1 = self.enc1(img) enc2 = self.enc2(self.pool(enc1)) enc3 = self.enc3(self.pool(enc2)) #Bottleneck dec3 = self.bottleneck(self.pool(enc3)) #Decoder Path dec2 = self.dec3(torch.cat([self.upconv3(dec3), enc3], dim=1)) dec1 = self.dec2(torch.cat([self.upconv2(dec2), enc2], dim=1)) out = self.dec1(torch.cat([self.upconv1(dec1), enc1], dim=1)) return self.final-conv(out) def conv-block(self, in-channels, out-channels): return nn.Sequential( nn.Conv2d(in-channels, out-channels, kernel-size=3, padding=1), nn.BatchNorm2d(out-channels), nn.ReLU() — location: [97431](kindle://book?action=open&asin=B0D2KFXWK8&location=97431) ^ref-30077

---
事前学習物体検出と同様に，エンコーダ（バックボーン）の大規模な事前学 習は非常に有効です．本書執筆時点 （2024年3 月）で優れた性能を達成して いる意味領域分割モデルのほとんどが，自己教師あり学習 （7章）もしくは画 像とテキストデータによるバックボーンの事前学習 （8 章）を行っています [199, 202, 233, 234]． — location: [100054](kindle://book?action=open&asin=B0D2KFXWK8&location=100054) ^ref-13762

---
なお，転移元の学習を事前学習 （pre-training）， 転移先での学習をファイン チューニング （fine-tuning） と呼びます．また，転移先のタスクのことを下流 タスク （downstream task） といいます．物体検出の例だと， ImageNetでの — location: [110541](kindle://book?action=open&asin=B0D2KFXWK8&location=110541) ^ref-49402

---
画像分類が事前学習に対応し，物体検出が下流タスクに対応します． — location: [110975](kindle://book?action=open&asin=B0D2KFXWK8&location=110975) ^ref-16886

---
MAEの概要図． — location: [120587](kindle://book?action=open&asin=B0D2KFXWK8&location=120587) ^ref-59097

---
V&Lタスクを正確に解くためには，全く異なる 2つのデータ表 現（画像と言語）を適切に結び付ける必要があります． 左の画像は [301]： CC BY 2.0 — location: [128451](kindle://book?action=open&asin=B0D2KFXWK8&location=128451) ^ref-1448

---
グリッド特徴は， CNNやViTが出力する中間特徴マップそのものを表しま す． — location: [128889](kindle://book?action=open&asin=B0D2KFXWK8&location=128889) ^ref-8478

---
8.3.2 事前学習の基本方針 V&Lモデルの事前学習では，画像・キャプションペアデータを用い，（手法 によって多少の差異はあれど）基本的には次のいずれかもしくは組み合わせた タスクを解くことを通じて，画像表現と言語表現のアライメントを図ります． •画像からキャプションを生成する． •画像からキャプションの一部を補完する． •画像とキャプションの 1対1 の対応関係を築く（検索や対比学習）． — location: [132821](kindle://book?action=open&asin=B0D2KFXWK8&location=132821) ^ref-34711

---
8.3.8 CLIP — location: [137628](kindle://book?action=open&asin=B0D2KFXWK8&location=137628) ^ref-46793

---
